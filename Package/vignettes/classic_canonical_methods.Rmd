---
title: "Classic canonical methods"
subtitle: "Partial least squares, canonical correlation, and reduced rank regression"
author: "Derek Beaton"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Classic canonical methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this vignette we cover the three "classical canonical" optimizations: partial least squares, canonical correlation, and reduced rank regression (a.k.a. redundancy analysis). From there we cover each of the three optimizations for variations of "two-table" analyses. Also for each of these optimiztions we show these optimizations through each of the "generalized PLS" algorithms: correlation, regression, and canonical. Finally we show a "bonus" set of analyses that are the discriminant forms of these methods.

For all examples assume that ${\mathbf X}$ and ${\mathbf Y}$ are some data matrices, and that ${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$ are the column-wise centered and scaled versions. 

## Three optimizations
<!-- need to recheck these and perhaps simplify the explanation -->
The three approaches optimize latent variables under the following analogies, the: 

* partial least squares optimization is covariance: ${\mathbf x}^{T}{\mathbf y}$ 

* canonical correlation optimization is correlation: $({\mathbf x}^{T}{\mathbf x})^{-\frac{1}{2}}({\mathbf x}^{T}{\mathbf y})({\mathbf y}^{T}{\mathbf y})^{-\frac{1}{2}}$ 

* reduced rank regression optimization is regression: $({\mathbf x}^{T}{\mathbf x})^{-\frac{1}{2}}({\mathbf x}^{T}{\mathbf y})$  


## Partial least squares


## Canonical correlation


## Reduced rank regression 


## Comparisons between the three


## Discriminant analyses
