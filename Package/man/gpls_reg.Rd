% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpls_reg.R
\name{gpls_reg}
\alias{gpls_reg}
\title{Generalized partial least squares "regression decomposition" (GPLSREG)}
\usage{
gpls_reg(X, Y, XLW = diag(nrow(X)), YLW = diag(nrow(Y)),
  XRW = diag(ncol(X)), YRW = diag(ncol(Y)), components = 0,
  tol = .Machine$double.eps)
}
\arguments{
\item{X}{Data matrix with \emph{I} rows and \emph{J} columns}

\item{Y}{Data matrix with \emph{I} rows and \emph{K} columns}

\item{XLW}{An \emph{I} by \emph{I} matrix of row weights for \code{X}. Default is \code{diag(nrow(X))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{YLW}{An \emph{I} by \emph{I} matrix of row weights for \code{Y}. Default is \code{diag(nrow(Y))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{XRW}{A \emph{J} by \emph{J} matrix of row weights for \code{X}. Default is \code{diag(ncol(X))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{YRW}{A \emph{K} by \emph{K} matrix of row weights for \code{Y}. Default is \code{diag(ncol(Y))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{components}{The number of components to return. If < 1 then the maximum components will be returned. Default = 0.}

\item{tol}{default is .Machine$double.eps. A parameter to pass through to \code{\link[GSVD]{gplssvd}}; eliminates singular values that are effectively zero (and thus drops null components).}
}
\value{
A list of outputs
\item{d}{A vector containing the singular values from each iteration.}
\item{u}{Left (rows) singular vectors.}
\item{v}{Right (columns) singular vectors. In GPLSREG sometimes called "weight matrix".}
\item{lx}{Latent variable scores for rows of \code{X}}
\item{ly}{Latent variable scores for rows of \code{Y}}
\item{p}{Left (rows) generalized singular vectors.}
\item{q}{Right (columns) generalized singular vectors.}
\item{fi}{Left (rows) component scores.}
\item{fj}{Right (columns) component scores.}
\item{tx}{X "Latent vectors": A normed version of \code{lx} for use in rebuilding \code{X} data}
\item{u_hat}{X "Loading matrix": A "predicted" version of \code{u} for use in rebuilding \code{X} data}
\item{betas}{"Regression weights": Akin to betas for use in rebuilding \code{Y}}
\item{X_reconstructeds}{A version of \code{X} reconstructed for each iteration (i.e., latent variable/component)}
\item{Y_reconstructeds}{A version of \code{Y} reconstructed for each iteration (i.e., latent variable/component)}
\item{X_residuals}{The residualized (i.e., \code{X - X_reconstructeds}) version of \code{X} for each iteration (i.e., latent variable/component)}
\item{Y_residuals}{The residualized (i.e., \code{Y - Y_reconstructeds}) version of \code{Y} for each iteration (i.e., latent variable/component)}
\item{r2_x}{Proporition of explained variance from \code{X} to each latent variable/component.}
\item{r2_y}{Proporition of explained variance from \code{Y} to each latent variable/component.}
\item{Y_reconstructed}{A version of \code{Y} reconstructed from all iterations (i.e., latent variables/components); see \code{components}.}
\item{Y_residual}{The residualized (i.e., \code{Y - Y_reconstructed} from all iterations (i.e., latent variables/components); see \code{components}.}
}
\description{
Computes generalized partial least squares "regression decomposition" between two data matrices.
GPLSREG allows for the use of left (row) and right (column) weights for each data matrix.
}
\examples{

 \dontrun{
 library(GSVD)
 data("wine", package = "GSVD")
 X <- scale(wine$objective)
 Y <- scale(wine$subjective)


 ## standard partial least squares "regression decomposition"
 #### the first latent variable from reg & cor & can are identical in all PLSs.
 gplsreg_pls_optimization <- gpls_reg(X, Y)

 ## partial least squares "regression decomposition"
 ### but with the optimization per latent variable of CCA
 #### because of optimization, this ends up identical to
 #### cca(X, Y, center_X = F, center_Y = F, scale_X = F, scale_Y = F)
 gplsreg_cca_optimization <- gpls_reg( t(MASS::ginv(X)), t(MASS::ginv(Y)),
      XRW = crossprod(X), YRW = crossprod(Y))

 ## partial least squares "regression decomposition"
 ### but with the optimization per latent variable of RRR/RDA
 #### because of optimization, this ends up identical to
 #### rrr(X, Y, center_X = F, center_Y = F, scale_X = F, scale_Y = F)
 #### or rda(X, Y, center_X = F, center_Y = F, scale_X = F, scale_Y = F)
 gplsreg_rrr_optimization <- gpls_reg( t(MASS::ginv(X)), Y, XRW = crossprod(X))

 rm(X)
 rm(Y)

 ## partial least squares-correspondence analysis "regression decomposition"
 #### the first latent variable from reg & cor & can are identical in all PLSs.
 data("snps.druguse", package = "GSVD")
 X <- make_data_disjunctive(snps.druguse$DATA1)
 Y <- make_data_disjunctive(snps.druguse$DATA2)

 X_ca_preproc <- ca_preproc(X)
 Y_ca_preproc <- ca_preproc(Y)

 gplsreg_plsca <- gpls_reg( X = X_ca_preproc$Z, Y = Y_ca_preproc$Z,
     XLW = diag(1/X_ca_preproc$m), YLW = diag(1/Y_ca_preproc$m),
     XRW = diag(1/X_ca_preproc$w), YRW = diag(1/Y_ca_preproc$w)
 )
 }

}
\references{
Beaton, D., ADNI, Saporta, G., Abdi, H. (2019). A generalization of partial least squares regression and correspondence analysis for categorical and mixed data: An application with the ADNI data. \emph{bioRxiv}, 598888.
}
\seealso{
\code{\link{gpls_can}} \code{\link{gpls_cor}} \code{\link{pls_reg}} \code{\link{plsca_reg}} \code{\link[GSVD]{gplssvd}}
}
\keyword{diagonalization,}
\keyword{least}
\keyword{multivariate,}
\keyword{partial}
\keyword{squares}
