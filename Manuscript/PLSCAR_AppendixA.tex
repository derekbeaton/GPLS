% !TeX program = pdfLaTeX
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{textcomp}
\usepackage[hyphens]{url} % not crucial - just used below for the URL
\usepackage{hyperref}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

%% load any required packages here




\usepackage{float}
\usepackage{bbold}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{xcolor}

\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Appendix A}

  \author{
        Derek Beaton \\
    Rotman Research Institute, Baycrest Health Sciences\\
     and \\     ADNI \thanks{Data used in preparation of this article were obtained from the
Alzheimer's Disease Neuroimaging Initiative (ADNI) database
(\url{http://adni.loni.usc.edu/}). As such, the investigators within the
ADNI contributed to the design and implementation of ADNI and/or
provided data but did not participate in analysis or writing of this
report. A complete listing of ADNI investigators can be found at
http://adni.loni.ucla.edu/wpcontent/uploads/how\_to\_apply/ADNI\_Acknowledgement\_List.pdf} \\
    ADNI\\
     and \\     Gilbert Saporta \\
    Conservatoire National des Arts et Metiers\\
     and \\     Herv√© Abdi \\
    Behavioral and Brain Sciences, The University of Texas at Dallas\\
      }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Appendix A}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
A
\end{abstract}

\noindent%
{\it Keywords:} generalized singular value decomposition, latent models, genetics, neuroimaging, canonical correlation analysis
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

This Appendix provides additional details on PLS-CA-R and, in
particular, a variety of extensions of and generalizations from
PLS-CA-R. We also provide additional details on some concepts, such as
the generalized singular value decomposition. As established in the main
text, PLS-CA-R provides a generalization of PLS-R for categorical and
mixed data. However, PLS-CA-R provides the basis for futher
generalizations that extend to other optimizations, alternate metrics,
different PLS algorithms, and even ridge-like regularization. In this
Appendix we explain those additional generalizations and variations
based on how we established PLS-CA-R in the main text.

First, we explain the relationship between the SVD and GSVD in a more
detail than in the main text. Following that, we extend the concept of
the GSVD triplet for PLS with what we call ``the GPLSSVD sextuplet''.
Second, we show how the GPLSSVD triplet allows us to perform PLS-CA-R,
as well as other cross-decomposition techniques, more easily. From
there, we use the GPLSSVD triplet as a way to further simplify the three
primary PLS algorithms (regression, correlation, canonical). Third we
provide a short discussion on how the GPLSSVD---which is inspired from
PLS-CA-R---gives us a more unified way to accomodate different
optimizations (e.g., partial least squares vs.~canonical correlation)
and different constraints or metrics. Finally, we present two ways to
perform a ridge-like regularization with an emphasis on PLS-CA-R, but
the regularization applies to any technique under the GPLSSVD framework.

\hypertarget{the-svd-gsvd-and-gplssvd}{%
\section{The SVD, GSVD, and GPLSSVD}\label{the-svd-gsvd-and-gplssvd}}

The SVD of \({\bf X}\) is \begin{equation}
{\bf X} = {\bf U}{\boldsymbol \Delta}{\bf V}^{T},
\end{equation} where
\({\bf U}^{T}{\bf U} = {\bf I} = {\bf V}^{T}{\bf V}\). The GSVD of
\({\bf X}\) is \begin{equation}
{\bf X} = {\bf P}{\boldsymbol \Delta}{\bf Q}^{T},
\end{equation} where
\({\bf P}^{T}{\bf M}{\bf P} = {\bf I} = {\bf Q}^{T}{\bf W}{\bf Q}\).
Practically, the GSVD is performed through the SVD as
\(\widetilde{\mathbf X} = {\mathbf M}^{\frac{1}{2}}{\mathbf X}{\mathbf W}^{\frac{1}{2}} = {\mathbf U} {\boldsymbol \Delta} {\mathbf V}^{T}\),
where the generalized singular vectors are computed from the singular
vectors as \({\mathbf P} = {\mathbf M}^{-\frac{1}{2}}{\mathbf U}\) and
\({\mathbf Q} = {\mathbf W}^{-\frac{1}{2}}{\mathbf V}\). The
relationship between the SVD and GSVD can be expressed through what we
decompose as \begin{equation}
\widetilde{\mathbf X} = {\mathbf M}^{\frac{1}{2}}{\mathbf X}{\mathbf W}^{\frac{1}{2}} \Longleftrightarrow {\mathbf X} = {\mathbf M}^{-\frac{1}{2}}\widetilde{\mathbf X}{\mathbf W}^{-\frac{1}{2}},
\end{equation} where
\({\mathbf U}^{T}{\mathbf U} = {\mathbf I} = {\mathbf V}^{T}{\mathbf V}\),
and like with the GSVD
\({\mathbf P}^{T}{\mathbf M}{\mathbf P} = {\mathbf I} = {\mathbf Q}^{T}{\mathbf W}{\mathbf Q}\).
As noted in the main text, the GSVD can be presented in ``triplet
notation'' as
\(\mathrm{GSVD(}{\mathbf M}, {\mathbf X}, {\mathbf W}\mathrm{)}\).

\hypertarget{from-gsvd-triplet-to-gplssvd-sextuplet}{%
\subsection{From GSVD triplet to GPLSSVD
sextuplet}\label{from-gsvd-triplet-to-gplssvd-sextuplet}}

We introduce an extension of the GSVD triplet for PLS, called the
``GPLSSVD sextuplet''. The GPLSSVD sextuplet helps us in two ways: (1)
it simplifies some of the notation and decomposition concepts, and (2)
it provides the basis for generalization of cross-decomposition methods
(e.g., canonical correlation, reduced rank regression).

The ``GPLSSVD sextuplet'' takes the form of
\(\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf X}, {\mathbf Z}_{\mathbf X}, {\mathbf W}_{\mathbf X}, {\mathbf M}_{\mathbf Y}, {\mathbf Z}_{\mathbf Y}, {\mathbf W}_{\mathbf Y} \mathrm{)}\)
and like the GSVD, decomposes a matrix
\({\mathbf Z}_{\mathbf R} = ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X})^{T} {\mathbf M}_{\mathbf Y}^{\frac{1}{2}} {\mathbf Z}_{\mathbf Y}\)
as

\begin{equation}
{\mathbf Z}_{\mathbf R} = {\mathbf P} {\boldsymbol \Delta} {\mathbf Q}^{T}
\textrm{ with }
{\mathbf P}^{T}{\mathbf W}_{\mathbf X}{\mathbf P} = {\mathbf I} =
{\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q}.
\end{equation} From this decomposition, the GPLSSVD creates latent
variables as \begin{equation}
{\mathbf L}_{\mathbf X} 
= {\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}{\mathbf P} 
\textrm{ and } 
{\mathbf L}_{\mathbf Y} = 
{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}{\mathbf Q}
\textrm{ where }
{\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} 
= {\boldsymbol \Delta}. 
\end{equation}

Alternatively, we can show the GPLSSVD through the SVD. Let us refer to
\({\widetilde{\mathbf Z}_{\mathbf X}} = {\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}}\)
and
\({\widetilde{\mathbf Z}_{\mathbf Y}} = {\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}}\),
where
\(\widetilde{\mathbf Z}_{\mathbf R} = {\widetilde{\mathbf Z}_{\mathbf X}}^{T}{\widetilde{\mathbf Z}_{\mathbf Y}} = ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}({\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}})\).
We decompose \(\widetilde{\mathbf Z}_{\mathbf R}\) as

\begin{equation}
\widetilde{\mathbf Z}_{\mathbf R} = {\mathbf U} {\boldsymbol \Delta} {\mathbf V}^{T}
\textrm{ with }
{\mathbf U}^{T}{\mathbf U} = {\mathbf I} =
{\mathbf V}^{T}{\mathbf V}.
\end{equation} We can also compute the latent variables as
\begin{equation}
{\mathbf L}_{\mathbf X} 
= \widetilde{\mathbf Z}_{\mathbf X}{\mathbf U} 
\textrm{ and } 
{\mathbf L}_{\mathbf Y} = 
\widetilde{\mathbf Z}_{\mathbf Y}{\mathbf V}
\textrm{ where }
{\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} 
= {\boldsymbol \Delta}. 
\end{equation}

Like with the SVD and GSVD, the GPLSSVD has the same orthogonality
constraints:
\({\mathbf U}^{T}{\mathbf U} = {\mathbf I} = {\mathbf V}^{T}{\mathbf V}\),
or
\({\mathbf P}^{T}{\mathbf M}_{\mathbf Y}{\mathbf P} = {\mathbf I} = {\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q}\).
And per usual, the relationship between the singular vectors are
\({\mathbf U} = {\mathbf W}_{\bf X}^{\frac{1}{2}}{\mathbf P} \Longleftrightarrow {\mathbf P} = {\mathbf M}_{\bf X}^{-\frac{1}{2}}{\mathbf U}\)
and
\({\mathbf V} = {\mathbf W}_{\bf Y}^{\frac{1}{2}}{\mathbf Q} \Longleftrightarrow {\mathbf Q} = {\mathbf W}_{\bf Y}^{-\frac{1}{2}}{\mathbf V}\).
{[}Component scores{]}

{[}Then a bunch of connections{]}

When \({\mathbf X}\) and \({\mathbf Y}\) are column-wise centered
normalized and when all constraint matrices are equal to the identity
matrix---i.e.,
\(\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{\mathbf X}, {\mathbf I}, {\mathbf I}, {\mathbf Z}_{\mathbf Y}, {\mathbf I} \mathrm{)}\)---the
GPLSSVD implements the ``PLS correlation'' decomposition
\citep{krishnan_partial_2011, bookstein1994partial, mcintosh_spatial_1996},
also known as PLSSVD \citep{tenenhaus_regression_1998}, co-inertia
\citep[\citet{dray2014}]{doledec1994}, and originally as Tucker's
interbattery factor analysis \citep{tucker_inter-battery_1958}. Finally,
we also introduce a small modification of the ``triplet'' and
``sextuplet'' notations as a ``quadruplet'' and a ``septuplet'' that
indicate the desired rank to be returned by the GSVD or GPLSSVD. For
example, if we want only one component from either approach we would
indicate the desired rank to return as
\(\mathrm{GSVD(} {\mathbf M}_{{\mathbf X}}, {\mathbf Z}_{\mathbf X}, {\mathbf W}_{{\mathbf X}}, 1 \mathrm{)}\)
and
\(\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf X}, {\mathbf Z}_{\mathbf X}, {\mathbf W}_{\mathbf X}, {\mathbf M}_{\mathbf Y}, {\mathbf Z}_{\mathbf Y}, {\mathbf W}_{\mathbf Y}, 1 \mathrm{)}\).
Both the GSVD and GPLSSVD in these cases would return only one set of
singular vectors, generalized singular vectors, and component scores,
and one singular value; for GPLSSVD it would return only one pair of
latent variables.

Similarly to the GSVD, let us refer to
\(\widetilde{{\mathbf Z}}_{\mathbf X} = {\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}}\)
and
\(\widetilde{{\mathbf Z}}_{\mathbf Y} = {\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}}\).

The GPLSSVD makes use of the SVD wherein
\(\widetilde{\mathbf Z}_{\mathbf R} = \widetilde{{\mathbf Z}}_{\mathbf X}^{T}\widetilde{{\mathbf Z}}_{\mathbf Y} = ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}({\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}}) = {\mathbf U} {\boldsymbol \Delta} {\mathbf V}^{T}\).
The GPLSSVD generalized singular vectors and component scores are
computed as
\({\mathbf P} = {\mathbf W}_{{\mathbf X}}^{-\frac{1}{2}}{\mathbf U}\)
and
\({\mathbf F}_{J} = {\mathbf W}_{{\mathbf X}}{\mathbf P}{\boldsymbol \Delta}\)
for the \(J\) columns of \({\mathbf X}\), and
\({\mathbf Q} = {\mathbf W}_{{\mathbf Y}}^{-\frac{1}{2}}{\mathbf V}\)
and
\({\mathbf F}_{K} = {\mathbf W}_{{\mathbf Y}}{\mathbf Q}{\boldsymbol \Delta}\)
for the \(K\) columns of \({\mathbf Y}\). Like with the SVD
\({\mathbf U}^{T}{\mathbf U} = {\mathbf I} = {\mathbf V}^{T}{\mathbf V}\),
and like with the GSVD
\({\mathbf P}^{T}{\mathbf M}_{\mathbf X}{\mathbf P} = {\mathbf I} = {\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q}\).

The GPLSSVD also produces scores for the \(I\) rows of each
matrix---usually called latent variables---as
\({\mathbf L}_{\mathbf X} = \widetilde{\mathbf Z}_{\mathbf X}{\mathbf U}\)
and
\({\mathbf L}_{\mathbf Y} = \widetilde{\mathbf Z}_{\mathbf Y}{\mathbf V}\)
where
\({\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} = {\boldsymbol \Delta}\).
By its definition, this GPLSSVD maximization of the latent
variables---i.e.,
\({\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} = {\boldsymbol \Delta}\)---is
the ``PLS correlation'' decomposition.

Specifically, if \({\mathbf X}\) and \({\mathbf Y}\) were each
column-wise centered and/or normalized, then
\(\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{\mathbf X}, {\mathbf I}, {\mathbf I}, {\mathbf Z}_{\mathbf Y}, {\mathbf I} \mathrm{)}\)
is PLS correlation (a.k.a. PLSSVD or Tucker's approach).

\hypertarget{pls-algorithms}{%
\section{PLS algorithms}\label{pls-algorithms}}

We present Algorithm \ref{algo:plscar} as a ``generalized partial least
squares regression'' by way of the GPLSSVD sextuplet. We discuss the
generalized aspects of the algorithm in more detail in Section
\ref{section:Disc}.

Though we have presented PLS-CA-R as a generalization of PLS-R that
accomodates virutally any data type (by way of CA), the way we
formalized PLS-CA-R---in Section \ref{section:plscar_form} and describe
its algorithm in Algorithm \ref{algo:plscar}---leads to further variants
and broader generalizations; generalizations that span various PLS, CA,
and related approaches, several typical PLS algorithms, a variety of
optimizations (e.g., canonical correlation), and ridge-like
regularization.

PLS correlation decomposition is a symmetric method where neither data
table plays a privileged (or predictive) role, and is performed with a
single pass of the singular value decomposition (SVD). PLS canonical
decomposition is also symmetric, but makes use of the SVD iteratively
and deflates both data tables in each iteration.

There exist three commonly used PLS algorithms: (1) PLS ``correlation''
decomposition
\citep{krishnan_partial_2011, bookstein1994partial, mcintosh_spatial_1996}---also
known as PLSSVD \citep{tenenhaus_regression_1998} or Tucker's
Interbattery Factor Analysis \citep{tucker_inter-battery_1958}, (2) PLS
``canonical'' decompostion \citep{tenenhaus_regression_1998}, and

\hypertarget{gpls-algorithms}{%
\subsection{GPLS algorithms}\label{gpls-algorithms}}

In general there exist three primary PLS algorithms: PLS correlation
decomposition \citep{bookstein1994partial, ketterlinus1989partial}
generally more known in neuroimaging
\citep{mcintosh_spatial_1996, mcintosh_partial_2004, krishnan_partial_2011}
which has numerous alternate names such as PLS-SVD and Tucker's
interbattery factor analysis \citep{tucker_inter-battery_1958} amongst
others \citep[see also][]{beaton_partial_2016}, PLS regression
decomposition (cf.~Section \ref{section:plscar_form} and also Algorithm
\ref{algo:plscar}) and the PLS canonical decomposition
\citep{tenenhaus_regression_1998, wegelin2000survey}, which is a
symmetric method with iterative deflation (i.e., it has features of both
PLS-C and PLS-R). Given the way in which we formalize PLS-CA-R---as a
generalized PLS-R---here we show how PLS-CA-R provides the basis of
generalizations of these three algorithms, as well as further
optimizations, similar to \citet{borga_unified_1992},
\citet{indahl2009canonical}, and \citet{de2019pls} but we do so in a
more comprehensive way that incorporates more methods than other
unification strategies, and we also do so in a way that accomodates
multiple data types. We refer to the three techniques under the umbrella
of generalized partial least squares (GPLS) as GPLS-COR, GPLS-REG, and
GPLS-CAN, for the ``correlation'', ``regression'', and ``canonical''
decompositions respectively. GPLS-COR and GPLS-CAN are symmetric
decomposition approaches where neither \({\mathbf Z}_{{\mathbf X}}\) nor
\({\mathbf Z}_{{\mathbf Y}}\) are privileged. GPLS-REG is an asymmetric
decomposition approach where \({\mathbf Z}_{{\mathbf X}}\) is
privileged. We present the GPLS-COR, GPLS-REG, and then GPLS-CAN
algorithms with their respective optimizations. We do so in the
previously mentioned order because GPLS-COR is used as the basis of all
three algorithms and GPLS-CAN shares features and concepts with both
GPLS-COR and GPLS-REG. For all of these we rely on the formlization of
PLS-CA-R we established in Section
\ref{section:plscar_form}---specifically for various mixed data types
under the \(\chi^2\) model (as used in CA).

The GPLS-COR decomposition is the simplest GPLS technique. It requires
only a single pass of the SVD---or in our case the GPLSSVD. There are no
explicit iterative steps in GPLS-COR. GPLS-COR takes as input the two
preprocessed matrices---\({\mathbf Z}_{\mathbf X}\) and
\({\mathbf Z}_{\mathbf Y}\)---and their respective row and column
weights: \({\mathbf M}_{\mathbf X}\) and \({\mathbf W}_{\mathbf X}\) for
\({\mathbf Z}_{\mathbf X}\), and \({\mathbf M}_{\mathbf Y}\) and
\({\mathbf W}_{\mathbf Y}\) for \({\mathbf Z}_{\mathbf Y}\), where \(C\)
is the desired number of components to return. GPLS-COR is shown in
Algorithm \ref{algo:plsc}.

\RestyleAlgo{boxed}
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Generalized PLS-correlation between ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{${\mathbf M}_{\mathbf{X}}$, ${\mathbf Z}_{{\mathbf X}}$, ${\mathbf W}_{\mathbf{X}}$, ${\mathbf M}_{\mathbf{Y}}$, ${\mathbf Z}_{{\mathbf Y}}$, ${\mathbf W}_{\mathbf{Y}}$, $C$}
\Output{${\mathbf U}$, ${\mathbf V}$, ${\mathbf P}$, ${\mathbf Q}$, ${\mathbf F}_{J}$, ${\mathbf F}_{K}$, ${\mathbf L}_{{\mathbf X}}$, ${\mathbf L}_{{\mathbf Y}}$, ${\boldsymbol \Delta}$}
\BlankLine
  $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf{X}}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\mathbf{X}}, {\mathbf M}_{\mathbf{Y}}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\mathbf{Y}}, C \mathrm{)}$ \\
\caption{Generalized PLS-correlation algorithm. GPLS-COR is the GPLSSVD and provides the basis of other GPLS techniques. Furthermore, GPLS-COR easily allows for a variety of optmizations for examples canonical correlation, reduced rank regression (redundancy analysis), and even ridge-like regularization.}
\label{algo:plsc}
\end{algorithm}

GPLS-COR maximizes the relationship between \({\mathbf L}_{\mathbf X}\)
and \({\mathbf L}_{\mathbf Y}\) with the orthogonality constraint
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0\)
when \(c \neq c'\) where
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}\)
and thus
\({\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y} = {\mathbf U}^{T}\widetilde{\mathbf Z}_{\mathbf X}^{T}\widetilde{\mathbf Z}_{\mathbf Y}{\mathbf V}^{T} = {\mathbf U}^{T}\widetilde{\mathbf Z}_{\mathbf R}{\mathbf V}^{T} = {\mathbf U}^{T}{\mathbf U}{\boldsymbol \Delta}{\mathbf V}^{T}{\mathbf V}^{T} = {\boldsymbol \Delta}\).
We can show this with the generalized vectors and weight as
\({\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y} = {\mathbf P}^{T}{\mathbf W}_{\mathbf X}{\mathbf Z}_{\mathbf X}^{T}{\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}{\mathbf Q}^{T} = {\mathbf P}^{T}{\mathbf W}_{\mathbf X}{\mathbf P}{\boldsymbol \Delta}{\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q} = {\boldsymbol \Delta}\).
Furthermore, GPLS-COR (via GPLSSVD) provides all of the other outputs as
previously described in Section \ref{section:GSVDCA}. GPLS-COR---which
is the GPLSSVD---provides the basis for the other two algorithms: both
GPLS-REG and GPLS-CAN make use of GPLS-COR (i.e., the GPLSSVD) with rank
1 solutions iteratively.

The GPLS-REG decomposition builds off of the GPLS-COR algorithm, but
does so by way of the GPLSSVD septuplet iteratively for \(C\)
iterations, with only a rank 1 solution is provided for each use of the
GPLSSVD. Then the two data matrices---\({\mathbf Z}_{\mathbf X}\) and
\({\mathbf Z}_{\mathbf Y}\)---are deflated for each step asymmetrically,
with a privileged \({\mathbf Z}_{\mathbf X}\). GPLS-REG is shown in
Algorithm \ref{algo:plscar}.

\RestyleAlgo{boxed}
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Generalized PLS-regression between ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{${\mathbf M}_{\mathbf{X}}$, ${\mathbf Z}_{{\mathbf X}}$, ${\mathbf W}_{\mathbf{X}}$, ${\mathbf M}_{\mathbf{Y}}$, ${\mathbf Z}_{{\mathbf Y}}$, ${\mathbf W}_{\mathbf{Y}}$, $C$}
\Output{$\widetilde{\mathbf U}$, $\widetilde{\mathbf V}$, $\widetilde{\mathbf P}$, $\widetilde{\mathbf Q}$, $\widetilde{\mathbf F}_{J}$, $\widetilde{\mathbf F}_{K}$, ${\mathbf L}_{{\mathbf X}}$, ${\mathbf L}_{{\mathbf Y}}$, $\widetilde{\boldsymbol \Delta}$, ${\mathbf T}_{{\mathbf X}}$, $\widehat{\mathbf U}$, ${\mathbf B}$}
\BlankLine
\For{$c=1, \dots, C$}{

  $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf{X}}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\mathbf{X}}, {\mathbf M}_{\mathbf{Y}}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\mathbf{Y}}, 1 \mathrm{)}$ \\
  ${\mathbf t}_{\mathbf X} \leftarrow {\boldsymbol \ell}_{\mathbf X} \times {{\lvert\lvert {\boldsymbol \ell}_{\mathbf X} \rvert\rvert}^{-1}}$\\
  $b \leftarrow {\boldsymbol \ell}_{\mathbf Y}^{T}{\mathbf t}_{\mathbf X}$\\
  $\widehat{\mathbf u} \leftarrow ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}{\mathbf t}_{\mathbf X}$\\
  ${\mathbf Z}_{{\mathbf X}} \leftarrow {\mathbf Z}_{{\mathbf X}} - [{\mathbf M}_{\mathbf X}^{-\frac{1}{2}}({\mathbf t}_{\mathbf X}\widehat{\mathbf u}^{T}){\mathbf W}_{\mathbf X}^{-\frac{1}{2}}]$\\
  ${\mathbf Z}_{{\mathbf Y}} \leftarrow {\mathbf Z}_{{\mathbf Y}} - [{\mathbf M}_{\mathbf Y}^{-\frac{1}{2}}(b{\mathbf t}_{\mathbf X}\widetilde{\mathbf{v}}^{T}){\mathbf W}_{\mathbf Y}^{-\frac{1}{2}}]$
}
\caption{Generalized PLS-regression algorithm. The results of a rank 1 GPLSSVD are used to compute the latent variables and values necessary for deflation of ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$. PLS-CA-R is a specific instance of GPLS-REG, which we defined in Section \ref{section:plscar_form}.}
\label{algo:plscar}
\end{algorithm}

GPLS-REG maximizes the relationship between \({\mathbf L}_{\mathbf X}\)
and \({\mathbf L}_{\mathbf Y}\) with the orthogonality constraint
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0\)
when \(c \neq c'\) where
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}\)
which is also
\(\mathrm{diag\{}{\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y}\mathrm{\}} = \mathrm{diag\{}\widetilde{\boldsymbol \Delta}\mathrm{\}}\).

The GPLS-CAN decomposition builds off of the GPLS-COR algorithm, but
does so by way of the GPLSSVD septuplet iteratively for \(C\)
iterations, with only a rank 1 solution is provided for each use of the
GPLSSVD. Then the two data matrices---\({\mathbf Z}_{\mathbf X}\) and
\({\mathbf Z}_{\mathbf Y}\)---are deflated for each step symmetrically.
GPLS-CAN is shown in Algorithm \ref{algo:plscacan}

\RestyleAlgo{boxed}
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Generalized PLS-canonical between ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{${\mathbf M}_{\mathbf{X}}$, ${\mathbf Z}_{{\mathbf X}}$, ${\mathbf W}_{\mathbf{X}}$, ${\mathbf M}_{\mathbf{Y}}$, ${\mathbf Z}_{{\mathbf Y}}$, ${\mathbf W}_{\mathbf{Y}}$, $C$}
\Output{$\widetilde{\mathbf U}$, $\widetilde{\mathbf V}$, $\widetilde{\mathbf P}$, $\widetilde{\mathbf Q}$, $\widetilde{\mathbf F}_{J}$, $\widetilde{\mathbf F}_{K}$, ${\mathbf L}_{{\mathbf X}}$, ${\mathbf L}_{{\mathbf Y}}$, $\widetilde{\boldsymbol \Delta}$, ${\mathbf T}_{{\mathbf X}}$, ${\mathbf T}_{{\mathbf Y}}$, $\widehat{\mathbf U}$, $\widehat{\mathbf V}$}
\BlankLine
\For{$c=1, \dots, C$}{

  $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf{X}}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\mathbf{X}}, {\mathbf M}_{\mathbf{Y}}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\mathbf{Y}}, 1 \mathrm{)}$ \\
  ${\mathbf t}_{\mathbf X} \leftarrow {\boldsymbol \ell}_{\mathbf X} \times {{\lvert\lvert {\boldsymbol \ell}_{\mathbf X} \rvert\rvert}^{-1}}$\\
  ${\mathbf t}_{\mathbf Y} \leftarrow {\boldsymbol \ell}_{\mathbf Y} \times {{\lvert\lvert {\boldsymbol \ell}_{\mathbf Y} \rvert\rvert}^{-1}}$\\
  $\widehat{\mathbf u} \leftarrow ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}{\mathbf t}_{\mathbf X}$\\
  $\widehat{\mathbf v} \leftarrow ({\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}})^{T}{\mathbf t}_{\mathbf Y}$\\  
  
  ${\mathbf Z}_{{\mathbf X}} \leftarrow {\mathbf Z}_{{\mathbf X}} - [{\mathbf M}_{\mathbf X}^{-\frac{1}{2}}({\mathbf t}_{\mathbf X}\widehat{\mathbf u}^{T}){\mathbf W}_{\mathbf X}^{-\frac{1}{2}}]$\\
   ${\mathbf Z}_{{\mathbf Y}} \leftarrow {\mathbf Z}_{{\mathbf Y}} - [{\mathbf M}_{\mathbf Y}^{-\frac{1}{2}}({\mathbf t}_{\mathbf Y}\widehat{\mathbf v}^{T}){\mathbf W}_{\mathbf Y}^{-\frac{1}{2}}]$
}
\caption{Generalized PLS-canonical algorithm. The results of a rank 1 GPLSSVD are used to compute the latent variables and values necessary for deflation of ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$. Note that the deflation in GPLS-CAN differs from GPLS-REG in Algorithm \ref{algo:plscar}.}
\label{algo:plscacan}
\end{algorithm}

GPLS-CAN maximizes the relationship between \({\mathbf L}_{\mathbf X}\)
and \({\mathbf L}_{\mathbf Y}\) with the orthogonality constraints
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0\)
and
\({\boldsymbol \ell}_{{\mathbf Y},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0\)
when \(c \neq c'\) where
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}\)
which is also
\(\mathrm{diag\{}{\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y}\mathrm{\}} = \mathrm{diag\{}\widetilde{\boldsymbol \Delta}\mathrm{\}}\).

Note that across all three algorithms defined here, that the first
component is identical when the same preprocessed data and constraints
are provided to the GPLSSVD. In nearly all cases, subsequent components
across the three algorithms differ, but also generally they do not
differ substantially. The similarities can be traced back to the common
maximization of
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}\),
where the differences can be traced back to the specific orthogonality
optimizations when \(c \neq c'\) where: (1) GPLS-COR in Algorithm
\ref{algo:plsc} is
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0\),
(2) GPLS-REG in Algorithm \ref{algo:plscar} is
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0\),
and (3) GPLS-CAN in Algorithm \ref{algo:plscacan} is both
\({\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0\)
and
\({\boldsymbol \ell}_{{\mathbf Y},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0\).

\hypertarget{gpls-optimizations-and-further-generalizations}{%
\subsection{GPLS optimizations and further
generalizations}\label{gpls-optimizations-and-further-generalizations}}

From the GPLS perspective, we can better unify the wide variety of
approaches with similar goals but variations of metrics,
transformations, and optimizations that often appear under a wide
variety of names (e.g., PLS, CCA, interbattery factor analysis,
co-inertia analysis, canonical variates, PLS-CA, and so on; see
\citet{abdi2017canonical}). The way we defined the GPLS algorithms---in
particular with the constraints applied to the rows and columns of each
data matrix---leads to numerous further generalizations.

For simplicity, let us first focus on Algorithm \ref{algo:plsc}, and
assume that \({\mathbf X}\) and \({\mathbf Y}\) are continuous data,
where \({\mathbf Z}_{\mathbf X}\) and \({\mathbf Z}_{\mathbf Y}\) are
column-wise centered and/or scaled versions of \({\mathbf X}\) and
\({\mathbf Y}\). Though we have established Algorithm \ref{algo:plsc} as
GPLS-COR---and more generally as the GPLSSVD---we can obtain the results
of three of the most common ``two-table'' techniques: PLS correlation
(PLSC), canonical correlation analysis (CCA), and redundancy analysis
(RDA, a.k.a., reduced rank regression {[}RRR{]}). Standard PLSC is
performed as
\(\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{{\mathbf X}}, {\mathbf I}, {\mathbf I}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf I}\mathrm{)}\),
CCA is performed as
\(\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{{\mathbf X}}, ({\mathbf Z}_{\mathbf X}^{T}{\mathbf Z}_{\mathbf X})^{-1}, {\mathbf I}, {\mathbf Z}_{{\mathbf Y}}, ({\mathbf Z}_{\mathbf Y}^{T}{\mathbf Z}_{\mathbf Y})^{-1}\mathrm{)}\),
and RDA---where \({\mathbf X}\) is privileged---is performed as
\(\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{{\mathbf X}}, ({\mathbf Z}_{\mathbf X}^{T}{\mathbf Z}_{\mathbf X})^{-1}, {\mathbf I}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf I}\mathrm{)}\).
Furthermore, these three variants---PLSC, CCA, and RDA/RRR---also
generalize discriminant analyses under different optimizations so long
as \({\mathbf X}\) is a dummy-coded or complete disjunctive matrix to
assign each observation (row) to a specific group or category (columns).

Most importantly, because of the ways we formalized the GPLS
algorithms---see also Section \ref{section:plscar_form}---and the
variety of ways to suitably transform data (e.g., the various coding
schemes we have shown) allow application of PLS-CA-R and GPLS algorithms
on a variety of different problems or models such as log or power
transformations and alternate choices for weights (see Eq.
\ref{eq:weightmats_v1}) or models (see Eq. \ref{eq:models}). That means
that the GPLS algorithms further generalize many approaches, especially
the numerous variants of CA. Generally in the cases of strictly positive
data, there may be a need to preprocess data within the family of power
transformations for CA \citep{greenacre2009power} or alternate distance
metrics such as Hellinger distance
\citep{rao1995review, escofier1978analyse}. Finally, with the choices of
weights can change, as they do for Hellinger CA, and for the variations
of ``non-symmetrical CA''
\citep{d1992non, kroonenberg1999nonsymmetric, takane1991relationships},
where both types of variants require one set of weights as
\({\mathbf I}\) (akin to RDA/RRR-type optimizations with CA/\(\chi^2\)
models across any of the GPLS algorithms).

\hypertarget{ridge-like-regularization}{%
\subsection{Ridge-like regularization}\label{ridge-like-regularization}}

It is also possible to apply ridge-like regularization to PLS-CA
regression, correlation, and canonical decompositions. We show two
possible strategies for ridge-like regularization under the data/model
assumptions and the preprocessing we established in Section
\ref{section:plscar_form}.

The first approach is based on Takane's regularized multiple CA
\citep{takane_regularized_2006} and regularized nonsymmetric CA
\citep{takane_regularized_2009-1}. To do so, it is convenient to
slightly reformulate PLS-CA-R, but we still require \({\mathbf X}\),
\({\mathbf Y}\), \({\mathbf O}_{\mathbf X}\),
\({\mathbf O}_{\mathbf Y}\), \({\mathbf E}_{\mathbf X}\), and
\({\mathbf E}_{\mathbf Y}\) as defined in Section
\ref{section:plscar_form}. First we re-define
\({\mathbf Z}_{\mathbf X} = ({\mathbf O}_{\mathbf X} - {\mathbf E}_{\mathbf X}) \times (\mathbf{1}^{T}{\mathbf X1})\)
and
\({\mathbf Z}_{\mathbf Y} = ({\mathbf O}_{\mathbf Y} - {\mathbf E}_{\mathbf Y}) \times (\mathbf{1}^{T}{\mathbf Y1})\);
which are the same as in Eq. \ref{eq:plscar_Zs} except scaled by the
grand sum of its respective source data matrix. Next we define the
following additional matrices:
\({\mathbf D}_{{\mathbf X},I} = \mathrm{diag\{ \mathbf{X1} \}}\), and
\({\mathbf D}_{{\mathbf Y},I} = \mathrm{diag\{ \mathbf{Y1} \}}\) which
are diagonal matrices of the row sums of \({\mathbf X}\) and
\({\mathbf Y}\), respectively and
\({\mathbf D}_{{\mathbf X},J} = \mathrm{diag\{ \mathbf{1}^{T} \mathbf{X} \}}\),
and
\({\mathbf D}_{{\mathbf Y},K} = \mathrm{diag\{ \mathbf{1}^{T}\mathbf{Y} \}}\)
which are the column sums of \({\mathbf X}\) and \({\mathbf Y}\). Then
PLS-CA correlation, regression, and canonical decompositions replace the
GPLSSVD step in Algorithms \ref{algo:plsc}, \ref{algo:plscar},
\ref{algo:plscacan} with
\(\mathrm{GPLSSVD(}{\mathbf D}_{{\mathbf X},I}^{-1},{\mathbf Z}_{\mathbf X}^{T}, {\mathbf D}_{{\mathbf X},J}^{-1}, {\mathbf D}_{{\mathbf Y},I}^{-1},{\mathbf Z}_{\mathbf Y}^{T}, {\mathbf D}_{{\mathbf Y},K}^{-1} \mathrm{)}\).
The only differences between this Takane-ian reformulation and what we
originally established is that the generalized singular vectors
(\({\mathbf P}\) and \({\mathbf Q}\)) and the component scores
(\({\mathbf F}_{\mathbf J}\) and \({\mathbf F}_{\mathbf K}\)) differ by
constant scaling factors (which are the sums of \({\mathbf X}\) and
\({\mathbf Y}\) for their respective scores).

We can regularize PLS-CA-R in the same way as Takane's RMCA. We require
(1) a ridge parameter which we refer to as \(\lambda\) and (2) variants
of \({\mathbf D}_{{\mathbf X},I}\), \({\mathbf D}_{{\mathbf X},J}\),
\({\mathbf D}_{{\mathbf Y},I}\), and \({\mathbf D}_{{\mathbf Y},K}\)
that we refer to as
\({\mathbb D}_{{\mathbf X},I} = {\mathbf D}_{{\mathbf X},I} + [\lambda \times ({\mathbf Z}_{\mathbf X}{\mathbf Z}_{\mathbf X}^{T})^{+}]\),
\({\mathbb D}_{{\mathbf Y},I} = {\mathbf D}_{{\mathbf Y},I} + [\lambda \times ({\mathbf Z}_{\mathbf Y}{\mathbf Z}_{\mathbf Y}^{T})^{+}]\),
\({\mathbb D}_{{\mathbf X},J} = {\mathbf D}_{{\mathbf X},J} + [\lambda \times {\mathbf Z}_{\mathbf X}^{T}({\mathbf Z}_{\mathbf X}{\mathbf Z}_{\mathbf X}^{T})^{+}{\mathbf Z}_{\mathbf X}]\),
and
\({\mathbb D}_{{\mathbf Y},K} = {\mathbf D}_{{\mathbf Y},K} + [\lambda \times {\mathbf Z}_{\mathbf Y}^{T}({\mathbf Z}_{\mathbf Y}{\mathbf Z}_{\mathbf Y}^{T})^{+}{\mathbf Z}_{\mathbf Y}]\).
When \(\lambda = 0\) then
\({\mathbb D}_{{\mathbf X},I} = {\mathbf D}_{{\mathbf X},I}\),
\({\mathbb D}_{{\mathbf Y},I} = {\mathbf D}_{{\mathbf Y},I}\),
\({\mathbb D}_{{\mathbf X},J} = {\mathbf D}_{{\mathbf X},J}\),
\({\mathbb D}_{{\mathbf Y},K} = {\mathbf D}_{{\mathbf Y},K}\). We obtain
regularized forms of PLS-CA for the correlation, regression, and
canonical decompositions if we simply replace the GPLSSVD step in each
algorithm with
\(\mathrm{GPLSSVD(}{\mathbb D}_{{\mathbf X},I}^{-1},{\mathbf Z}_{\mathbf X}^{T}, {\mathbb D}_{{\mathbf X},J}^{-1}, {\mathbb D}_{{\mathbf Y},I}^{-1},{\mathbf Z}_{\mathbf Y}^{T}, {\mathbb D}_{{\mathbf Y},K}^{-1} \mathrm{)}\).
As per Takane's recommendation \citep{takane_regularized_2006},
\(\lambda\) could be any positive value, though integers in the range
from 1 to 20 provide sufficient regularization, especially as
\(\lambda\) increases.

But the Takane-ian approach may not be feasible when \(I\), \(J\),
and/or \(K\) are particularly large because the various crossproduct and
projection matrices require a large amount of memory and/or
computational expense. So we now introduce a ``truncated'' version of
the Takane regularization which is more computationally efficient, and
analogous to the regularization procedure of Allen
\citep{allen_sparse_2013, allen_generalized_2014}. We re-define
\({\mathbb D}_{{\mathbf X},I} = {\mathbf D}_{{\mathbf X},I} + (\lambda \times {\mathbf I})\)
and
\({\mathbb D}_{{\mathbf Y},I} = {\mathbf D}_{{\mathbf Y},I} + (\lambda \times {\mathbf I})\)
and then also
\({\mathbb D}_{{\mathbf X},J} = {\mathbf D}_{{\mathbf X},J} + (\lambda \times {\mathbf I})\)
and
\({\mathbb D}_{{\mathbf Y},K} = {\mathbf D}_{{\mathbf Y},K} + (\lambda \times {\mathbf I})\)
where \({\mathbf I}\) are identity matrices (\(1\)s on the diagonal) of
appropriate size. Like in the previous formulation, we replace the
values we have in the GPLSSVD step where
\(\mathrm{GPLSSVD(}{\mathbb D}_{{\mathbf X},I}^{-1},{\mathbf Z}_{\mathbf X}^{T}, {\mathbb D}_{{\mathbf X},J}^{-1}, {\mathbb D}_{{\mathbf Y},I}^{-1},{\mathbf Z}_{\mathbf Y}^{T}, {\mathbb D}_{{\mathbf Y},K}^{-1} \mathrm{)}\);
and in this particular case, the constraint matrices are all diagonal
matrices, which allows for a lower memory footprint and less
computational burden.

Finally, we have two concluding remarks on ridge-like regularization.
The first point is that the more simplified Takane/Allen hybrid approach
to ridge-like regularization also applies much more generally to
virtually any technique for the SVD or GPLSSVD. For any approach, we
only require some inflation factor (\(\lambda\)) for the constraints
alon the diagonals. The second point is that while we have presented
ridge-like regularization with a single \(\lambda\) it is entirely
possible to use different \(\lambda\)s for each set of constraints. But
even though it is possible, we do not recommend this approach, as it
would require a complex grid search over all the various \(\lambda\)
parameters. Alternatively, if multiple \(\lambda\)s were used, one could
minimize the number of parameters to search and set some of the
\(\lambda\)s to 0 and, for example, use only one or two \(\lambda\)
values instead of four possible \(\lambda\) values.

\hypertarget{fin}{%
\section{Fin}\label{fin}}

While that was the bulk of this work, our secondary goal was to further
generalize the PLS-CA approach and to better unify many methods under a
simpler framework, specifically by way of the GPLSSVD and our three GPLS
algorithms. Thus our generalizations---first established in Section
\ref{section:plscar_form}, and expanded upon in Discussion---accomodate:
almost any data type, various metrics (e.g., Hellinger distance),
various optimizations (e.g., PLS, CCA, or RDA type optmizations), and
even two strategies for ridge-like regularization. We have foregone any
discussions of inference, stability, and resampling for PLS-CA-R
because, as a generalization of PLS-R, many inference and stability
approaches still apply---such as feature selection or sparsification
\citep{sutton_sparse_2018}, additional regularization or sparsification
approaches
\citep{le_floch_significant_2012-1, guillemot2019constrained, tenenhaus_variable_2014, tenenhaus_regularized_2011},
cross-validation
\citep{wold_principal_1987, rodriguez-perez_overoptimism_2018, kvalheim_number_2019, abdi_partial_2010-1},
permutation \citep{berry_permutation_2011}, various bootstrap
\citep{efron_bootstrap_1979, chernick_bootstrap_2008} approaches
\citep{abdi_partial_2010-1, takane_regularized_2009-1} or tests
\citep{mcintosh_partial_2004, krishnan_partial_2011}, and other
frameworks such as split-half resampling
\citep{strother_quantitative_2002-1, kovacevic2013revisiting, strother2004optimizing}---and
are easily adapted for the PLS-CA-R and GPLS frameworks.

\bibliographystyle{agsm}
\bibliography{plscar.bib}

\end{document}
