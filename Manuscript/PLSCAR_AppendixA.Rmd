---
title: "Supplemental Material for 'A generalization of partial least squares regression and correspondence analysis for categorical and mixed data: An application with the ADNI data'"

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Derek Beaton
  # thanks: Acknowledgements?
  affiliation: Rotman Research Institute, Baycrest Health Sciences

- name: ADNI
  affiliation: ADNI
  thanks: Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu/). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not  participate in  analysis  or  writing  of  this  report. A  complete  listing  of  ADNI  investigators can be found at http\://adni.loni.ucla.edu/wpcontent/uploads/how\_to\_apply/ADNI\_Acknowledgement\_List.pdf
  
- name: Gilbert Saporta
  affiliation: Conservatoire National des Arts et Metiers
  
- name: Herv√© Abdi
  affiliation: Behavioral and Brain Sciences, The University of Texas at Dallas
  

keywords:
- generalized partial least squares
- canonical correlation analysis
- reduced rank regression
- ridge regularization
- R package

abstract: |
  We provide supplemental material for partial least squares-correspondence analysis-regression (PLS-CA-R_ that highlights how PLS-CA-R provides the baiss for generalization of numerous cross-decomposition methods (e.g., PLS, CCA, RRR) as well as ridge-like regularization. Here we provide additional material and explanations (e.g., algorithms) based on the formulation in the main text.
    
bibliography: plscar.bib
output: rticles::asa_article
header-includes: 
  - \usepackage{float}
  - \usepackage{bbold}
  - \usepackage{subfig}
  - \usepackage{graphicx}
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
  - \usepackage{booktabs}
  - \usepackage{algorithm2e}
  - \usepackage{caption}
  - \usepackage{tabularx}
  - \usepackage{verbatim}
  - \usepackage{xcolor}
---

```{r paper_setup, echo=F, include=F}
library(knitr)
library(kableExtra)
```


# Introduction

This Appendix provides additional details on PLS-CA-R and, in particular, a variety of extensions of and generalizations from PLS-CA-R. We also provide additional details on some concepts, such as the generalized singular value decomposition. As established in the main text, PLS-CA-R provides a generalization of PLS-R for categorical and mixed data. However, PLS-CA-R provides the basis for futher generalizations that extend to other optimizations, alternate metrics, different PLS algorithms, and even ridge-like regularization. In this Appendix we explain those additional generalizations and variations based on how we established PLS-CA-R in the main text. 


First, we explain the relationship between the SVD and GSVD in a more detail than in the main text. Following that, we extend the concept of the GSVD triplet for PLS with what we call "the GPLSSVD sextuplet". Second, we show how the GPLSSVD triplet allows us to perform PLS-CA-R, as well as other cross-decomposition techniques, more easily. From there, we use the GPLSSVD triplet as a way to further simplify the three primary PLS algorithms (regression, correlation, canonical). Third we provide a short discussion on how the GPLSSVD---which is inspired from PLS-CA-R---gives us a more unified way to accomodate different optimizations (e.g., partial least squares vs. canonical correlation) and different weights or metrics. We then present two ways to perform a ridge-like regularization with an emphasis on PLS-CA-R, but the regularization applies to any technique under the GPLSSVD framework. Finally, we point out numerous strategies and approaches for inference and stability assessment that are easily adapted for PLS-CA-R specifically and GPLSSVD generally.


## Additional remarks

Given our formalization of PLS-CA-R, we want to point out variations, some caveats, and some additional features. We also briefly discuss here how PLS-CA-R provides the basis for numerous variations and broader generalizations (we go into much more detail in subsequent sections).

In PLS-CA-R (and PLS-R) each subsequent $\widetilde{\delta}$ is not guaranteed to be smaller than the previous, with the exception that all $\widetilde \delta$ are smaller than the first. This is a by-product of the iterative process and the deflation step. This problem poses two issues: (1) visualization of component scores and (2) explained variance. For visualization of the component scores---which use $\widetilde \delta$---there is an alternative computation: ${\mathbf F}^{'}_{J} = {\mathbf W}_{J}^{-1}\widetilde{\mathbf P}$ and ${\mathbf F}^{'}_{K} = {\mathbf W}_{K}^{-1}\widetilde{\mathbf Q}$. This alternative is referred to as "asymmetric component scores" in the correspondence analysis literature [@abdi2014correspondence; @greenacre1993biplots]. Additionally, instead of computing the variance per component or latent variable, we can instead compute the amount of variance explained by each component in $\mathbf X$ and $\mathbf Y$. To do so we require the sum of the eigenvalues of each of the respective matrices per iteration via CA (with the GSVD). Before the first iteration of PLS-CA-R we obtain the full variance (i.e., the sum of the eigenvalues) of each matrix from $\mathrm{GSVD(} {\mathbf M}^{-1}_{\mathbf X}, {\mathbf Z}_{\mathbf X}, {\mathbf W}^{-1}_{\mathbf X} \mathrm{)}$ and $\mathrm{GSVD(} {\mathbf M}^{-1}_{\mathbf Y}, {\mathbf Z}_{\mathbf Y}, {\mathbf W}^{-1}_{\mathbf Y} \mathrm{)}$, which we respectively refer to as ${\phi}_{\bf X}$ and ${\phi}_{\bf Y}$ We can compute the sum of the eigenvalues for each deflated version of ${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$ through the GSVD just as above, referred to as ${\phi}_{{\bf X},c}$ and ${\phi}_{{\bf Y},c}$. For each $c$ component the proportion of explained variance for each matrix is $\frac{{\phi}_{\bf X} - {\phi}_{{\bf X},c}}{{\phi}_{\bf X}}$ and $\frac{{\phi}_{\bf Y} - {\phi}_{{\bf Y},c}}{{\phi}_{\bf Y}}$.


The weights we use are derived from the $\chi^2$ assumption of independence. However nearly any choices of weights could be used, so long as the weight matrices are at least positive semi-definite (which requires the use of a generalized inverse). If alternate row weights (i.e., ${\mathbf M}_{\mathbf X}$ or ${\mathbf M}_{\mathbf Y}$) were chosen, then the fitted values and residuals are no longer guaranteed to be orthogonal (the same condition is true in weighted OLS).  

PLS-CA-R provides an important basis for various extensions that allow for example, to accommodate the PLS-SVD and canonical PLS algorithms. Our formulation of PLS-CA-R provides the basis for, and leads to a more generalized approach for other cross-decomposition techniques or optimizations (e.g., canonical correlation, reduced rank regression). This basis also allows for the use of alternate metrics as well as ridge-like regularization. We show how PLS-CA-R leads to these generalizations in the Supplemental Material.

Though we formalized PLS-CA-R as a method for categorical (nominal) data coded in complete disjunctive format (as seen in Table \ref{table:disj}---see SEX columns---or Table \ref{table:snps_models_disj}), PLS-CA-R can easily accommodate various data types without loss of information. Specifically, both continuous and ordinal data can be handled with relative ease and in a "pseudo-disjunctive" format, also referred to as "fuzzy coding" where complete disjunctive would be a "crisp coding" [@greenacrefuzzy]. We explain exactly how to handle various data types as Section \ref{section:appex} progresses, which reflects more "real world" problems: complex, mixed data types, and multi-source data.

# The SVD, GSVD, and GPLSSVD

The SVD of ${\bf X}$ is
\begin{equation}
{\bf X} = {\bf U}{\boldsymbol \Delta}{\bf V}^{T},
\end{equation}
where ${\bf U}^{T}{\bf U} = {\bf I} = {\bf V}^{T}{\bf V}$. The GSVD of ${\bf X}$ is
\begin{equation}
{\bf X} = {\bf P}{\boldsymbol \Delta}{\bf Q}^{T},
\end{equation}
where ${\bf P}^{T}{\bf M}{\bf P} = {\bf I} = {\bf Q}^{T}{\bf W}{\bf Q}$. Practically, the GSVD is performed through the SVD as $\widetilde{\mathbf X} = {\mathbf M}^{\frac{1}{2}}{\mathbf X}{\mathbf W}^{\frac{1}{2}} = {\mathbf U} {\boldsymbol \Delta} {\mathbf V}^{T}$, where the generalized singular vectors are computed from the singular vectors as ${\mathbf P} = {\mathbf M}^{-\frac{1}{2}}{\mathbf U}$ and ${\mathbf Q} = {\mathbf W}^{-\frac{1}{2}}{\mathbf V}$. The relationship between the SVD and GSVD can be expressed through what we decompose as
\begin{equation}
\widetilde{\mathbf X} = {\mathbf M}^{\frac{1}{2}}{\mathbf X}{\mathbf W}^{\frac{1}{2}} \Longleftrightarrow {\mathbf X} = {\mathbf M}^{-\frac{1}{2}}\widetilde{\mathbf X}{\mathbf W}^{-\frac{1}{2}}.
\end{equation}
As noted in the main text, the GSVD can be presented in "triplet notation" as $\mathrm{GSVD(}{\mathbf M}, {\mathbf X}, {\mathbf W}\mathrm{)}$.


## From GSVD triplet to GPLSSVD sextuplet 

We introduce an extension of the GSVD triplet for PLS, called the "GPLSSVD sextuplet". The GPLSSVD sextuplet helps us in two ways: (1) it simplifies some of the notation and decomposition concepts, and (2) it provides the basis for generalization of cross-decomposition methods (e.g., canonical correlation, reduced rank regression).

The "GPLSSVD sextuplet" takes the form of $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf X}, {\mathbf Z}_{\mathbf X}, {\mathbf W}_{\mathbf X}, {\mathbf M}_{\mathbf Y}, {\mathbf Z}_{\mathbf Y}, {\mathbf W}_{\mathbf Y} \mathrm{)}$ and like the GSVD, decomposes a matrix ${\mathbf Z}_{\mathbf R} = ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X})^{T}  {\mathbf M}_{\mathbf Y}^{\frac{1}{2}} {\mathbf Z}_{\mathbf Y}$ as

\begin{equation}
{\mathbf Z}_{\mathbf R} = {\mathbf P} {\boldsymbol \Delta} {\mathbf Q}^{T}
\textrm{ with }
{\mathbf P}^{T}{\mathbf W}_{\mathbf X}{\mathbf P} = {\mathbf I} =
{\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q}.
\end{equation}
From this decomposition, the GPLSSVD produces latent variables as 
\begin{equation}
{\mathbf L}_{\mathbf X} 
= {\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}{\mathbf P} 
\textrm{ and } 
{\mathbf L}_{\mathbf Y} = 
{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}{\mathbf Q}
\textrm{ where }
{\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} 
= {\boldsymbol \Delta}. 
\end{equation}

Alternatively, we can show the GPLSSVD through the SVD. Let us refer to ${\widetilde{\mathbf Z}_{\mathbf X}} = {\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}}$ and ${\widetilde{\mathbf Z}_{\mathbf Y}} = {\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}}$, where $\widetilde{\mathbf Z}_{\mathbf R} = {\widetilde{\mathbf Z}_{\mathbf X}}^{T}{\widetilde{\mathbf Z}_{\mathbf Y}} = ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}({\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}})$. We decompose $\widetilde{\mathbf Z}_{\mathbf R}$ as

\begin{equation}
\widetilde{\mathbf Z}_{\mathbf R} = {\mathbf U} {\boldsymbol \Delta} {\mathbf V}^{T}
\textrm{ with }
{\mathbf U}^{T}{\mathbf U} = {\mathbf I} =
{\mathbf V}^{T}{\mathbf V}.
\end{equation}
We can also compute the latent variables as
\begin{equation}
{\mathbf L}_{\mathbf X} 
= \widetilde{\mathbf Z}_{\mathbf X}{\mathbf U} 
\textrm{ and } 
{\mathbf L}_{\mathbf Y} = 
\widetilde{\mathbf Z}_{\mathbf Y}{\mathbf V}
\textrm{ where }
{\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} 
= {\boldsymbol \Delta}. 
\end{equation}


Like with the SVD and GSVD, the GPLSSVD has the same orthogonality constraints: ${\mathbf U}^{T}{\mathbf U} = {\mathbf I} = {\mathbf V}^{T}{\mathbf V}$, or ${\mathbf P}^{T}{\mathbf M}_{\mathbf Y}{\mathbf P} = {\mathbf I} = {\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q}$. The relationship between the singular vectors are ${\mathbf U} = {\mathbf W}_{\bf X}^{\frac{1}{2}}{\mathbf P} \Longleftrightarrow {\mathbf P} = {\mathbf M}_{\bf X}^{-\frac{1}{2}}{\mathbf U}$ and ${\mathbf V} = {\mathbf W}_{\bf Y}^{\frac{1}{2}}{\mathbf Q} \Longleftrightarrow {\mathbf Q} = {\mathbf W}_{\bf Y}^{-\frac{1}{2}}{\mathbf V}$. We can also compute component (a.k.a. factor) scores from the GPLSSVD as ${\bf F}_{J} = {\mathbf W}_{\bf X}{\mathbf P}{\boldsymbol \Delta}$ and ${\bf F}_{K} = {\mathbf W}_{\bf Y}{\mathbf Q}{\boldsymbol \Delta}$. An alternate form of the component scores are ${\bf F}_{J}' = {\mathbf W}_{\bf X}{\mathbf P}$ and ${\bf F}_{K}' = {\mathbf W}_{\bf Y}{\mathbf Q}$.


Finally, we note one last extension to the GPLSSVD to make it the "GPLSSVD septuplet" (similarly we also a "GSVD quadruplet"). We can include desired rank to return as an input parameter for the GPLSSVD septuplet (and GSVD quadruplet) as $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf X}, {\mathbf Z}_{\mathbf X}, {\mathbf W}_{\mathbf X}, {\mathbf M}_{\mathbf Y}, {\mathbf Z}_{\mathbf Y}, {\mathbf W}_{\mathbf Y}, C \mathrm{)}$ (and $\mathrm{GSVD(} {\mathbf M}_{\mathbf X}, {\mathbf Z}_{\mathbf X}, {\mathbf W}_{\mathbf X}, C \mathrm{)}$), where $C$ is an integer to indicate the rank of the solution desired. For example, when $C = 1$ the GPLSSVD (and GSVD) return a rank 1 solution. The $C$ parameter could be any value (constrained to the minimum rank of either ${\bf X}$ or ${\bf Y}$), but $C=1$ is particlarly convenient for our following generalizations.

# PLS and GPLS algorithms

Though we have presented PLS-CA regression as a generalization of PLS regression that accomodates virutally any data type (by way of CA), the way we formalized PLS-CA regression leads to further variants and broader generalizations. These generalizations span (1) various PLS, CA, and related approaches, (2) several typical PLS algorithms, (3) a variety of optimizations (e.g., canonical correlation), and (4) ridge-like regularization.

There exist three commonly used PLS algorithms: (1) PLS regression (PLS-REG) decomposition [@wold1975soft; @wold_collinearity_1984; @wold_pls-regression_2001; @abdi_partial_2010-1], (2) PLS correlation (PLS-COR) decomposition [@bookstein1994partial; @ketterlinus1989partial] generally more known in neuroimaging [@mcintosh_spatial_1996; @mcintosh_partial_2004; @krishnan_partial_2011] and also has numerous alternate names such as PLS-SVD, co-inertia [@doledec1994,@dray2014], and Tucker's interbattery factor analysis [@tucker_inter-battery_1958] amongst others [see also @beaton_partial_2016], and (3) PLS canonical (PLS-CAN) decomposition [@tenenhaus_regression_1998; @wegelin2000survey] which is a symmetric method (like PLS-COR) with iterative deflation (like PLS-REG).

<!-- PLS correlation (PLS-COR) decomposition is a symmetric method where neither data table plays a privileged (or predictive) role. PLS-COR is typically performed as a single pass of the SVD. PLS canonical (PLS-CAN) decomposition is also symmetric, but makes is iterative, and deflates both data tables in each iteration. PLS regression (PLS-REG) is an asymmetric method where a set of ${\bf Y}$ responses are predicted from a set of ${\bf X}$ predictors. -->


Based on how we formalized PLS-CA regression, we now show how PLS-CA regression provides the basis of generalizations of these three algorithms, as well as further optimizations, similar to @borga_unified_1992, @indahl2009canonical, and @de2019pls. But we do so in a more comprehensive way that incorporates more methods than other unification strategies, and we also do so in a way that accomodates multiple data types. We refer to the generalization of the three previously mentioned PLS techniques under the umbrella of generalized partial least squares (GPLS) as GPLS-COR, GPLS-REG, and GPLS-CAN, for the "correlation", "regression", and "canonical" decompositions respectively. GPLS-COR and GPLS-CAN are symmetric decomposition approaches where neither ${\mathbf X}$ nor ${\mathbf Y}$ are privileged. GPLS-REG is an asymmetric decomposition approach where ${\mathbf X}$ is privileged. We present the GPLS-COR, GPLS-REG, and then GPLS-CAN algorithms with their respective optimizations. We do so in the previously mentioned order because GPLS-COR---by way of the GPLSSVD---is used as the basis of all three algorithms and GPLS-CAN shares features and concepts with both GPLS-COR and GPLS-REG. For all of these we rely on the formlization of PLS-CA regression as established in the main text.


## GPLS-COR

The GPLS-COR decomposition is the simplest GPLS technique. It requires only a single pass of the SVD---or in our case the GPLSSVD. There are no explicit iterative steps in GPLS-COR. GPLS-COR takes as input the two preprocessed matrices---${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$---and their respective row and column weights: ${\mathbf M}_{\mathbf X}$ and ${\mathbf W}_{\mathbf X}$ for ${\mathbf Z}_{\mathbf X}$, and ${\mathbf M}_{\mathbf Y}$ and ${\mathbf W}_{\mathbf Y}$ for ${\mathbf Z}_{\mathbf Y}$, where $C$ is the desired number of components to return. GPLS-COR is shown in Algorithm \ref{algo:plsc}.


<!-- The GPLSSVD also produces scores for the $I$ rows of each matrix---usually called latent variables---as ${\mathbf L}_{\mathbf X} = \widetilde{\mathbf Z}_{\mathbf X}{\mathbf U}$ and ${\mathbf L}_{\mathbf Y} = \widetilde{\mathbf Z}_{\mathbf Y}{\mathbf V}$ where ${\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} = {\boldsymbol \Delta}$. By its definition, this GPLSSVD maximization of the latent variables---i.e., ${\mathbf L}_{\mathbf X}^{T} {\mathbf L}_{\mathbf Y} = {\boldsymbol \Delta}$---is the "PLS correlation" decomposition. -->


<!-- Specifically, if ${\mathbf X}$ and ${\mathbf Y}$ were each column-wise centered and/or normalized, then $\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{\mathbf X}, {\mathbf I}, {\mathbf I}, {\mathbf Z}_{\mathbf Y}, {\mathbf I} \mathrm{)}$ is PLS correlation (a.k.a. PLSSVD or Tucker's approach).  -->

\RestyleAlgo{boxed}
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Generalized PLS-correlation between ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{${\mathbf M}_{\mathbf{X}}$, ${\mathbf Z}_{{\mathbf X}}$, ${\mathbf W}_{\mathbf{X}}$, ${\mathbf M}_{\mathbf{Y}}$, ${\mathbf Z}_{{\mathbf Y}}$, ${\mathbf W}_{\mathbf{Y}}$, $C$}
\Output{${\boldsymbol \Delta}$, ${\mathbf U}$, ${\mathbf V}$, ${\mathbf P}$, ${\mathbf Q}$, ${\mathbf F}_{J}$, ${\mathbf F}_{K}$, ${\mathbf L}_{{\mathbf X}}$, ${\mathbf L}_{{\mathbf Y}}$}
\BlankLine
  $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf{X}}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\mathbf{X}}, {\mathbf M}_{\mathbf{Y}}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\mathbf{Y}}, C \mathrm{)}$ \\
\caption{Generalized PLS-correlation algorithm. GPLS-COR is the GPLSSVD and provides the basis of other GPLS techniques. Furthermore, GPLS-COR easily allows for a variety of optmizations for examples canonical correlation, reduced rank regression (redundancy analysis), and even ridge-like regularization, which then extend to the other GPLS algorithms (i.e., regression and canonical decompositions). Note that this is a truncated version of the algorithm and does not include all of the GPLSSVD outputs.}
\label{algo:plsc}
\end{algorithm}

GPLS-COR maximizes the relationship between ${\mathbf L}_{\mathbf X}$ and ${\mathbf L}_{\mathbf Y}$ with the orthogonality constraint ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0$ when $c \neq c'$ where ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}$ and thus ${\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y} = {\mathbf U}^{T}\widetilde{\mathbf Z}_{\mathbf X}^{T}\widetilde{\mathbf Z}_{\mathbf Y}{\mathbf V}^{T} = {\mathbf U}^{T}\widetilde{\mathbf Z}_{\mathbf R}{\mathbf V}^{T} = {\mathbf U}^{T}{\mathbf U}{\boldsymbol \Delta}{\mathbf V}^{T}{\mathbf V}^{T} = {\boldsymbol \Delta}$. We can also show this with the generalized vectors and constraints as ${\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y} = {\mathbf P}^{T}{\mathbf W}_{\mathbf X}{\mathbf Z}_{\mathbf X}^{T}{\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}{\mathbf Q}^{T} = {\mathbf P}^{T}{\mathbf W}_{\mathbf X}{\mathbf P}{\boldsymbol \Delta}{\mathbf Q}^{T}{\mathbf W}_{\mathbf Y}{\mathbf Q} = {\boldsymbol \Delta}$. 


The primary example of GPLS-COR is the standard "PLS correlation" approach. Let's assume that ${\mathbf X}$ and ${\mathbf Y}$ are comprised of continuous data. So, ${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$ are column-wise centered (and/or normalized) versions of ${\mathbf X}$ and ${\mathbf Y}$. When all weight matrices are identity matrices, then the GPLSSVD implements the "PLS correlation" decomposition as $\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{\mathbf X}, {\mathbf I}, {\mathbf I}, {\mathbf Z}_{\mathbf Y}, {\mathbf I} \mathrm{)}$ and also as shown in Algorithm \ref{algo:plsc}. However, the way we established the GPLSSVD and Algorithm \ref{algo:plsc} allows us to obtain the results of three of the most common cross-decomposition ("two-table") techniques: PLS correlation (shown above), reduced rank regression (RRR, a.k.a., reduced rank regression [RDA]), and canonical correlation analysis (CCA). RRR/RDA is performed as $\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{{\mathbf X}}, ({\mathbf Z}_{\mathbf X}^{T}{\mathbf Z}_{\mathbf X})^{-1}, {\mathbf I}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf I}\mathrm{)}$ and CCA is performed as $\mathrm{GPLSSVD(} {\mathbf I}, {\mathbf Z}_{{\mathbf X}}, ({\mathbf Z}_{\mathbf X}^{T}{\mathbf Z}_{\mathbf X})^{-1}, {\mathbf I}, {\mathbf Z}_{{\mathbf Y}}, ({\mathbf Z}_{\mathbf Y}^{T}{\mathbf Z}_{\mathbf Y})^{-1}\mathrm{)}$. In Table \ref{table:crossdecomp}, we show how these three techniques are related through the GPLSSVD by highlighting that they are merely a change in choice of weights (metrics).


\begin{table}
\centering
\begin{tabular}{ l  c  c  c  c }
  Method & ${\bf M}_{\bf X}$ & ${\bf W}_{\bf X}$ & ${\bf M}_{\bf Y}$ & ${\bf W}_{\bf Y}$ \\
  \hline			
  PLS-COR & ${\bf I}$ & ${\bf I}$ & ${\bf I}$ & ${\bf I}$ \\
  RRR/RDA & ${\bf I}$ & $({\mathbf Z}_{\mathbf X}^{T}{\mathbf Z}_{\mathbf X})^{-1}$ & ${\bf I}$ & ${\bf I}$ \\
  CCA & ${\bf I}$ & $({\mathbf Z}_{\mathbf X}^{T}{\mathbf Z}_{\mathbf X})^{-1}$ & ${\bf I}$ & $({\mathbf Z}_{\mathbf Y}^{T}{\mathbf Z}_{\mathbf Y})^{-1}$ \\
  \hline  
\end{tabular}
\caption{The primary cross-decomposition techniques framed as GPLSSVD approaches.}\label{table:crossdecomp}
\end{table}


Furthermore, these three variants---PLSC, CCA, and RDA/RRR---also generalize discriminant analyses under different optimizations so long either ${\mathbf X}$ or ${\mathbf Y}$ (depending on the technique) is a dummy-coded (complete disjunctive) matrix, where each observation (row) is assigned to a specific group or category (columns). Going further, if we have disjunctive (or pseudo-disjunctive) data---as we do in the main text--- we can use GPLSSVD to obtain the results of PLS-CA "correlation" [@beaton_partial_2016]. Using the same matrices as established in the main text, PLS-CA "correlation" is $\mathrm{GPLSSVD(} {\mathbf M}_{\bf X}^{-1}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\bf X}^{-1}, {\mathbf M}_{\bf Y}^{-1}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\bf Y}^{-1}\mathrm{)}$, where ${\mathbf M}_{\bf X}$ and ${\mathbf M}_{\bf Y}$ are diagonal matrices of row frequencies for each matrix, where ${\mathbf W}_{\bf X}$ and ${\mathbf W}_{\bf Y}$ are diagonal matrices of column frequencies for each matrix, and ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$ are the deviations from independence matrices.

GPLS-COR---which is just the GPLSSVD---provides the basis for the other two algorithms: both GPLS-REG and GPLS-CAN make use of GPLS-COR (i.e., the GPLSSVD) with rank 1 solutions iteratively.


## GPLS-REG

The GPLS-REG decomposition uses the GPLSSVD septuplet iteratively for $C$ iterations, with only a rank 1 solution is provided for each use of the GPLSSVD. Alternatively, GPLS-REG can be thought of a direct extension of GPLS-COR as defined in Algorithm \ref{algo:plsc}.

Then the two data matrices---${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$---are deflated for each step asymmetrically, with a privileged ${\mathbf Z}_{\mathbf X}$. GPLS-REG is shown in Algorithm \ref{algo:plscar}.

\RestyleAlgo{boxed}
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Generalized PLS-regression between ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{${\mathbf M}_{\mathbf{X}}$, ${\mathbf Z}_{{\mathbf X}}$, ${\mathbf W}_{\mathbf{X}}$, ${\mathbf M}_{\mathbf{Y}}$, ${\mathbf Z}_{{\mathbf Y}}$, ${\mathbf W}_{\mathbf{Y}}$, $C$}
\Output{$\widetilde{\boldsymbol \Delta}$, $\widetilde{\mathbf U}$, $\widetilde{\mathbf V}$, $\widetilde{\mathbf P}$, $\widetilde{\mathbf Q}$, $\widetilde{\mathbf F}_{J}$, $\widetilde{\mathbf F}_{K}$, ${\mathbf L}_{{\mathbf X}}$, ${\mathbf L}_{{\mathbf Y}}$, ${\mathbf T}_{{\mathbf X}}$, $\widehat{\mathbf U}$, ${\mathbf B}$}
\BlankLine
\For{$c=1, \dots, C$}{

  $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf{X}}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\mathbf{X}}, {\mathbf M}_{\mathbf{Y}}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\mathbf{Y}}, 1 \mathrm{)}$ \\
  ${\mathbf t}_{\mathbf X} \leftarrow {\boldsymbol \ell}_{\mathbf X} \times {{\lvert\lvert {\boldsymbol \ell}_{\mathbf X} \rvert\rvert}^{-1}}$\\
  $b \leftarrow {\boldsymbol \ell}_{\mathbf Y}^{T}{\mathbf t}_{\mathbf X}$\\
  $\widehat{\mathbf u} \leftarrow ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}{\mathbf t}_{\mathbf X}$\\
  ${\mathbf Z}_{{\mathbf X}} \leftarrow {\mathbf Z}_{{\mathbf X}} - [{\mathbf M}_{\mathbf X}^{-\frac{1}{2}}({\mathbf t}_{\mathbf X}\widehat{\mathbf u}^{T}){\mathbf W}_{\mathbf X}^{-\frac{1}{2}}]$\\
  ${\mathbf Z}_{{\mathbf Y}} \leftarrow {\mathbf Z}_{{\mathbf Y}} - [{\mathbf M}_{\mathbf Y}^{-\frac{1}{2}}(b{\mathbf t}_{\mathbf X}{\mathbf{v}}^{T}){\mathbf W}_{\mathbf Y}^{-\frac{1}{2}}]$
}
\caption{Generalized PLS-regression algorithm. The results of a rank 1 GPLSSVD are used to compute the latent variables and values necessary for deflation of ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$. Note that this is a truncated version of the algorithm and does not include all of the GPLSSVD outputs.}
\label{algo:plscar}
\end{algorithm}


GPLS-REG maximizes the relationship between ${\mathbf L}_{\mathbf X}$ and ${\mathbf L}_{\mathbf Y}$ with the orthogonality constraint ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0$ when $c \neq c'$ where ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}$ which is also  $\mathrm{diag\{}{\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y}\mathrm{\}} = \mathrm{diag\{}\widetilde{\boldsymbol \Delta}\mathrm{\}}$. When ${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$ are column-wise centered (and/or normalized) versions of ${\mathbf X}$ and ${\mathbf Y}$, and when all weight matrices are identity matrices, then GPLS-REG is (one of the) traditional "PLS regression" decomposition(s; akin to the SIMPLS algorithm in @tenenhaus_regression_1998). We detail the regression decomposition for categorical and mixed data in the main text.


## GPLS-CAN

The GPLS-CAN decomposition can be thought of as a compromise between GPLS-COR and GPLS-REG, it: (1) is symmetric like GPLS-COR, and (2) uses the GPLSSVD septuplet iteratively for $C$ iterations---with only a rank 1 solution is provided for each use of the GPLSSVD---like GPLS-REG. In GPLS-CAN, the two data matrices---${\mathbf Z}_{\mathbf X}$ and ${\mathbf Z}_{\mathbf Y}$---are deflated for each iteration. GPLS-CAN is shown in Algorithm \ref{algo:plscacan}

\RestyleAlgo{boxed}
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Generalized PLS-canonical between ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{${\mathbf M}_{\mathbf{X}}$, ${\mathbf Z}_{{\mathbf X}}$, ${\mathbf W}_{\mathbf{X}}$, ${\mathbf M}_{\mathbf{Y}}$, ${\mathbf Z}_{{\mathbf Y}}$, ${\mathbf W}_{\mathbf{Y}}$, $C$}
\Output{$\widetilde{\mathbf U}$, $\widetilde{\mathbf V}$, $\widetilde{\mathbf P}$, $\widetilde{\mathbf Q}$, $\widetilde{\mathbf F}_{J}$, $\widetilde{\mathbf F}_{K}$, ${\mathbf L}_{{\mathbf X}}$, ${\mathbf L}_{{\mathbf Y}}$, $\widetilde{\boldsymbol \Delta}$, ${\mathbf T}_{{\mathbf X}}$, ${\mathbf T}_{{\mathbf Y}}$, $\widehat{\mathbf U}$, $\widehat{\mathbf V}$}
\BlankLine
\For{$c=1, \dots, C$}{

  $\mathrm{GPLSSVD(} {\mathbf M}_{\mathbf{X}}, {\mathbf Z}_{{\mathbf X}}, {\mathbf W}_{\mathbf{X}}, {\mathbf M}_{\mathbf{Y}}, {\mathbf Z}_{{\mathbf Y}}, {\mathbf W}_{\mathbf{Y}}, 1 \mathrm{)}$ \\
  ${\mathbf t}_{\mathbf X} \leftarrow {\boldsymbol \ell}_{\mathbf X} \times {{\lvert\lvert {\boldsymbol \ell}_{\mathbf X} \rvert\rvert}^{-1}}$\\
  ${\mathbf t}_{\mathbf Y} \leftarrow {\boldsymbol \ell}_{\mathbf Y} \times {{\lvert\lvert {\boldsymbol \ell}_{\mathbf Y} \rvert\rvert}^{-1}}$\\
  $\widehat{\mathbf u} \leftarrow ({\mathbf M}_{\mathbf X}^{\frac{1}{2}}{\mathbf Z}_{\mathbf X}{\mathbf W}_{\mathbf X}^{\frac{1}{2}})^{T}{\mathbf t}_{\mathbf X}$\\
  $\widehat{\mathbf v} \leftarrow ({\mathbf M}_{\mathbf Y}^{\frac{1}{2}}{\mathbf Z}_{\mathbf Y}{\mathbf W}_{\mathbf Y}^{\frac{1}{2}})^{T}{\mathbf t}_{\mathbf Y}$\\  
  
  ${\mathbf Z}_{{\mathbf X}} \leftarrow {\mathbf Z}_{{\mathbf X}} - [{\mathbf M}_{\mathbf X}^{-\frac{1}{2}}({\mathbf t}_{\mathbf X}\widehat{\mathbf u}^{T}){\mathbf W}_{\mathbf X}^{-\frac{1}{2}}]$\\
   ${\mathbf Z}_{{\mathbf Y}} \leftarrow {\mathbf Z}_{{\mathbf Y}} - [{\mathbf M}_{\mathbf Y}^{-\frac{1}{2}}({\mathbf t}_{\mathbf Y}\widehat{\mathbf v}^{T}){\mathbf W}_{\mathbf Y}^{-\frac{1}{2}}]$
}
\caption{Generalized PLS-canonical algorithm. The results of a rank 1 GPLSSVD are used to compute the latent variables and values necessary for deflation of ${\mathbf Z}_{{\mathbf X}}$ and ${\mathbf Z}_{{\mathbf Y}}$. The deflation in GPLS-CAN is the same as in GPLS-REG in Algorithm \ref{algo:plscar}, but for both matrices. Note that this is a truncated version of the algorithm and does not include all of the GPLSSVD outputs.}
\label{algo:plscacan}
\end{algorithm}

GPLS-CAN maximizes the relationship between ${\mathbf L}_{\mathbf X}$ and ${\mathbf L}_{\mathbf Y}$ with the orthogonality constraints ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0$ and ${\boldsymbol \ell}_{{\mathbf Y},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0$ when $c \neq c'$ where ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}$ which is also  $\mathrm{diag\{}{\mathbf L}_{\mathbf X}^{T}{\mathbf L}_{\mathbf Y}\mathrm{\}} = \mathrm{diag\{}\widetilde{\boldsymbol \Delta}\mathrm{\}}$.

## GPLS algorithms summary

Note that across the three algorithms defined here, that the first component is identical when the same preprocessed data and weights (a.k.a. metrics) are provided to the GPLSSVD. In many cases, subsequent components across the three algorithms differ, but generally do not differ substantially. The similarities are because of the common approach to maximization: ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c} = \delta_{c}$. The differences are because of the different orthogonality constraints when $c \neq c'$ where: (1) GPLS-COR in Algorithm \ref{algo:plsc} is ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0$, (2) GPLS-REG in Algorithm \ref{algo:plscar} is ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0$, and (3) GPLS-CAN in Algorithm \ref{algo:plscacan} is both ${\boldsymbol \ell}_{{\mathbf X},c}^{T}{\boldsymbol \ell}_{{\mathbf X},c'} = 0$ and ${\boldsymbol \ell}_{{\mathbf Y},c}^{T}{\boldsymbol \ell}_{{\mathbf Y},c'} = 0$.


## GPLS optimizations and further generalizations

From the GPLS perspective, we can better unify the wide variety of approaches with similar goals but variations of metrics, transformations, and optimizations that often appear under a wide variety of names (e.g., PLS, CCA, interbattery factor analysis, co-inertia analysis, canonical variates, PLS-CA, and so on; see @abdi2017canonical). The way we defined the GPLS algorithms---in particular with the weights applied to the rows and columns of each data matrix---leads to numerous further generalizations. 

Given the way we establish the GPLS algorithms here, we provide the basis for further generalizations of many approaches. This is especially true for the numerous variants of correspondecne analysis, such as power transformations for CA [@greenacre2009power] alternate distance metrics such as Hellinger distances [@rao1995review; @escofier1978analyse], or "non-symmetrical CA" [@d1992non; @kroonenberg1999nonsymmetric; @takane1991relationships]. For many of these approaches, the weight matrices---either for the rows or columns of a given matrix---are what change between techniques. In some of the aforementioned cases (e.g., power transformations) there are also additional steps to preprocess the data.


## Ridge-like regularizations

We show two possible strategies for ridge-like regularization for the GPLSSVD (which then applies to any of the algorithms we outline above). We first show these two regularization approaches specifically for the PLS-CA framework. From there we briefly discuss how these regularization approaches extend to other techniques (e.g., CCA, PLS-COR) under the GPLSSVD framework. 


The first approach is based on Takane's regularized multiple CA [@takane_regularized_2006] and regularized nonsymmetric CA [@takane_regularized_2009-1]. To do so, it is convenient to slightly reformulate PLS-CA-R, but we still require ${\mathbf X}$, ${\mathbf Y}$, ${\mathbf O}_{\mathbf X}$, ${\mathbf O}_{\mathbf Y}$, ${\mathbf E}_{\mathbf X}$, and ${\mathbf E}_{\mathbf Y}$ as defined in the main text. First we re-define ${\mathbf Z}_{\mathbf X} = ({\mathbf O}_{\mathbf X} - {\mathbf E}_{\mathbf X}) \times (\mathbf{1}^{T}{\mathbf X1})$ and ${\mathbf Z}_{\mathbf Y} = ({\mathbf O}_{\mathbf Y} - {\mathbf E}_{\mathbf Y}) \times (\mathbf{1}^{T}{\mathbf Y1})$. Next we define the following additional matrices: ${\mathbf D}_{{\mathbf X},I} = \mathrm{diag\{ \mathbf{X1} \}}$, and ${\mathbf D}_{{\mathbf Y},I} = \mathrm{diag\{ \mathbf{Y1} \}}$ which are diagonal matrices of the row sums of ${\mathbf X}$ and ${\mathbf Y}$, and ${\mathbf D}_{{\mathbf X},J} = \mathrm{diag\{ \mathbf{1}^{T} \mathbf{X} \}}$, and ${\mathbf D}_{{\mathbf Y},K} = \mathrm{diag\{ \mathbf{1}^{T}\mathbf{Y} \}}$ which are the column sums of ${\mathbf X}$ and ${\mathbf Y}$. Then PLS-CA correlation, regression, and canonical decompositions replace the GPLSSVD step in Algorithms \ref{algo:plsc}, \ref{algo:plscar}, \ref{algo:plscacan} with $\mathrm{GPLSSVD(}{\mathbf D}_{{\mathbf X},I}^{-1},{\mathbf Z}_{\mathbf X}^{T}, {\mathbf D}_{{\mathbf X},J}^{-1}, {\mathbf D}_{{\mathbf Y},I}^{-1},{\mathbf Z}_{\mathbf Y}^{T}, {\mathbf D}_{{\mathbf Y},K}^{-1} \mathrm{)}$. The only differences between this reformulation and what we originally established is that the generalized singular vectors (${\mathbf P}$ and ${\mathbf Q}$) and the component scores (${\mathbf F}_{\mathbf J}$ and ${\mathbf F}_{\mathbf K}$) differ by constant scaling factors (which are the sums of ${\mathbf X}$ and ${\mathbf Y}$ for their respective scores).


We can regularize PLS-CA-R in the same way as Takane's RMCA. We require (1) a ridge parameter which we refer to as $\epsilon$ and (2) variants of ${\mathbf D}_{{\mathbf X},I}$, ${\mathbf D}_{{\mathbf X},J}$, ${\mathbf D}_{{\mathbf Y},I}$, and ${\mathbf D}_{{\mathbf Y},K}$ that we refer to as ${\mathbb D}_{{\mathbf X},I} = {\mathbf D}_{{\mathbf X},I} + [\epsilon \times ({\mathbf Z}_{\mathbf X}{\mathbf Z}_{\mathbf X}^{T})^{+}]$, ${\mathbb D}_{{\mathbf Y},I} = {\mathbf D}_{{\mathbf Y},I} + [\epsilon \times ({\mathbf Z}_{\mathbf Y}{\mathbf Z}_{\mathbf Y}^{T})^{+}]$, ${\mathbb D}_{{\mathbf X},J} = {\mathbf D}_{{\mathbf X},J} + [\epsilon \times {\mathbf Z}_{\mathbf X}^{T}({\mathbf Z}_{\mathbf X}{\mathbf Z}_{\mathbf X}^{T})^{+}{\mathbf Z}_{\mathbf X}]$, and ${\mathbb D}_{{\mathbf Y},K} = {\mathbf D}_{{\mathbf Y},K} + [\epsilon \times {\mathbf Z}_{\mathbf Y}^{T}({\mathbf Z}_{\mathbf Y}{\mathbf Z}_{\mathbf Y}^{T})^{+}{\mathbf Z}_{\mathbf Y}]$. When $\epsilon = 0$ then ${\mathbb D}_{{\mathbf X},I} = {\mathbf D}_{{\mathbf X},I}$, ${\mathbb D}_{{\mathbf Y},I} = {\mathbf D}_{{\mathbf Y},I}$, ${\mathbb D}_{{\mathbf X},J} = {\mathbf D}_{{\mathbf X},J}$, ${\mathbb D}_{{\mathbf Y},K} = {\mathbf D}_{{\mathbf Y},K}$. We obtain ridge-like regularized forms of PLS-CA for the correlation, regression, and canonical decompositions if we replace the GPLSSVD step in (or just the input to) each algorithm with $\mathrm{GPLSSVD(}{\mathbb D}_{{\mathbf X},I}^{-1},{\mathbf Z}_{\mathbf X}^{T}, {\mathbb D}_{{\mathbf X},J}^{-1}, {\mathbb D}_{{\mathbf Y},I}^{-1},{\mathbf Z}_{\mathbf Y}^{T}, {\mathbb D}_{{\mathbf Y},K}^{-1} \mathrm{)}$. As per Takane's recommendation [@takane_regularized_2006], $\epsilon$ could be any positive value, though integers in the range from 1 to 20 provide sufficient regularization.


However, the above approach may not be feasible when $I$, $J$, and/or $K$ are particularly large because the various crossproduct and projection matrices require a large amount of memory and/or computational expense. So,we can use a "truncated" version of the Takane regularization which is more computationally efficient, and analogous to the regularization procedure of Allen [@allen_sparse_2013; @allen_generalized_2014]. We re-define ${\mathbb D}_{{\mathbf X},I} = {\mathbf D}_{{\mathbf X},I} + (\epsilon \times {\mathbf I})$ and ${\mathbb D}_{{\mathbf Y},I} = {\mathbf D}_{{\mathbf Y},I} + (\epsilon \times {\mathbf I})$ and then also ${\mathbb D}_{{\mathbf X},J} = {\mathbf D}_{{\mathbf X},J} + (\epsilon \times {\mathbf I})$ and ${\mathbb D}_{{\mathbf Y},K} = {\mathbf D}_{{\mathbf Y},K} + (\epsilon \times {\mathbf I})$ where ${\mathbf I}$ are identity matrices of appropriate size. Like in the previous formulation, we replace the values we have in the GPLSSVD step where $\mathrm{GPLSSVD(}{\mathbb D}_{{\mathbf X},I}^{-1},{\mathbf Z}_{\mathbf X}^{T}, {\mathbb D}_{{\mathbf X},J}^{-1}, {\mathbb D}_{{\mathbf Y},I}^{-1},{\mathbf Z}_{\mathbf Y}^{T}, {\mathbb D}_{{\mathbf Y},K}^{-1} \mathrm{)}$; and in this particular case, the weight matrices are all diagonal matrices, which allows for a lower memory footprint and less computational burden.


We have two concluding remarks on the ridge-like regularizations we presented. First, though the above are presented under teh PLS-CA frameworks, the basis of these concepts extend to any of the GPLSSVD techniques. In particular, the more simplified Takane/Allen hybrid approach to ridge-like regularization is easier to generally apply: it requires only some inflation factor (i.e., $\epsilon$) along the diagonals of the weight matrices. However, the first (Takane's) approach was established in a framework more akin to CCA, and thus could also be used for any of the optimization approaches outlined in Table \ref{table:crossdecomp}. Second, though we presented ridge-like regularization with a single $\epsilon$ it is entirely possible to use different $\epsilon$s for each set of weights. Although it is possible, we do not necessarily recommend this approach, as it requires a (potentially expensive) grid search over all the various $\epsilon$ parameters. Alternatively, if multiple $\epsilon$s were used, one could minimize the number of parameters to search and set some of the $\epsilon$s to 0 and, for example, use only one or two $\epsilon$ values instead of four possible $\epsilon$ values.


## Implementation of algorithms

We provide an `R` package that implements all of the algorithms, with the cross-decomposition variations here: https://github.com/derekbeaton/gpls. This package provides direct interfaces to PLS-COR, PLS-REG, PLS-CAN, CCA, RRR, and PLS-CA-COR, PLS-CA-REG, and PLS-CA-CAN, as well as the generalized PLS approaches as outlined above.

# Further extensions of PLS-CA-R and GPLSSVD

In both the main text and here, we have foregone any discussions of inference, stability, and resampling for PLS-CA-R (or GPLSSVD) in part because many of the inference and stability approaches established throughout the broader PLS (and CCA) literature still apply with little or no changes. Such approaches include feature selection or sparsification [@sutton_sparse_2018], additional regularization or sparsification approaches [@le_floch_significant_2012-1; @guillemot2019constrained; @tenenhaus_variable_2014; @tenenhaus_regularized_2011], cross-validation [@wold_principal_1987; @rodriguez-perez_overoptimism_2018; @kvalheim_number_2019; @abdi_partial_2010-1], permutation [@berry_permutation_2011; @winkler2020permutation], various bootstrap approaches [@abdi_partial_2010-1; @takane_regularized_2009-1] or tests [@mcintosh_partial_2004; @krishnan_partial_2011], and other frameworks such as split-half resampling [@strother_quantitative_2002-1; @kovacevic2013revisiting; @strother2004optimizing].